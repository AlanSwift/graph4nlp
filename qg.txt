**************** MODEL CONFIGURATION ****************
batch_size               -->  80
beam_size                -->  5
coverage_loss_ratio      -->  0.3
dataset                  -->  squad-split2
decoder_args             -->  {'rnn_decoder_share': {'rnn_type': 'lstm', 'input_size': 300, 'hidden_size': 300, 'rnn_emb_input_size': 300, 'use_copy': False, 'use_coverage': False, 'graph_pooling_strategy': 'max', 'attention_type': 'sep_diff_encoder_type', 'fuse_strategy': 'concatenate', 'dropout': 0.3}, 'rnn_decoder_private': {'max_decoder_step': 41, 'node_type_num': None, 'tgt_emb_as_output_layer': True, 'teacher_forcing_rate': 0.8}}
decoder_name             -->  stdrnn
early_stop_metric        -->  BLEU_4
enc_rnn_dropout          -->  0.3
epochs                   -->  100
gpu                      -->  -1
grad_clipping            -->  10
graph_construction_args  -->  {'graph_construction_share': {'graph_type': 'dependency', 'root_dir': 'examples/pytorch/question_generation/data/squad_split2', 'topology_subdir': 'DependencyGraph', 'share_vocab': True, 'thread_number': 4, 'port': 9000, 'timeout': 15000}, 'graph_construction_private': {'edge_strategy': 'homogeneous', 'merge_strategy': 'tailhead', 'sequential_link': True, 'as_node': False, 'nlp_tools_args': {'name': 'stanfordcorenlp', 'tokenizer_args': {'whitespace': True}, 'normalize_parentheses': False, 'normalize_other_brackets': False, 'split_hyphenated': False, 'is_oneSentence': True, 'port': 9000, 'timeout': 15000}}, 'node_embedding': {'input_size': 300, 'hidden_size': 300, 'word_dropout': 0.2, 'rnn_dropout': 0.3, 'fix_bert_emb': False, 'fix_word_emb': True, 'embedding_style': {'single_token_item': True, 'emb_strategy': 'w2v_bilstm', 'num_rnn_layers': 1, 'bert_model_name': None, 'bert_lower_case': None}}}
graph_construction_name  -->  dependency
graph_embedding_args     -->  {'graph_embedding_share': {'num_layers': 3, 'input_size': 300, 'hidden_size': 300, 'output_size': 300, 'direction_option': 'undirected', 'feat_drop': 0.2}, 'graph_embedding_private': {'n_etypes': 1, 'bias': True, 'use_edge_weight': True}}
graph_embedding_name     -->  ggnn
lr                       -->  0.001
lr_patience              -->  3
lr_reduce_factor         -->  0.7
min_word_freq            -->  1
n_samples                -->  None
no_cuda                  -->  False
num_hidden               -->  300
num_workers              -->  10
out_dir                  -->  out/squad_split2/qg_ckpt_dep_ggnn
patience                 -->  30
pretrained_word_emb_name -->  840B
seed                     -->  1234
share_vocab              -->  True
top_word_vocab           -->  70000
word_dropout             -->  0.4
word_emb_size            -->  300
**************** MODEL CONFIGURATION ****************
[ Using CUDA ]

out/squad_split2/qg_ckpt_dep_ggnn
