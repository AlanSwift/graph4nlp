## Where the samples will be written
save_data: text_cnn/run/example
## Where the vocab(s) will be written
src_vocab: text_cnn/run/example.vocab.src
tgt_vocab: text_cnn/run/example.vocab.src
# Prevent overwriting existing files in the folder
overwrite: False

# truncate examples
src_seq_length_trunc: 400
tgt_seq_length_trunc: 100

# common vocabulary for source and target
share_vocab: True

# Corpus opts:
data:
    text_cnn:
        path_src: text_cnn/train.input
        path_tgt: text_cnn/train.output
    valid:
        path_src: text_cnn/val.input
        path_tgt: text_cnn/val.output

# maximum vocab size
src_vocab_size: 50000
tgt_vocab_size: 50000

#src_vocab: text_cnn/run/example.vocab.src
#tgt_vocab: text_cnn/run/example.vocab.tgt

save_model: text_cnn/run/model
copy_attn: false
global_attention: mlp
word_vec_size: 128
rnn_size: 512
layers: 1
encoder_type: brnn
train_steps: 200000
max_grad_norm: 2
dropout: 0
batch_size: 32
valid_batch_size: 32
optim: adam
learning_rate: 0.0005
adagrad_accumulator_init: 0.1
reuse_copy_attn: false
copy_loss_by_seqlength: false
bridge: true
seed: 777
world_size: 1
gpu_ranks: [0]

#onmt_translate -gpu 2 \
#               -batch_size 30 \
#               -beam_size 1 \
#               -model text_cnn/run/model_step_40000.pt \
#               -src text_cnn/test.input \
#               -output text_cnn/test_pred.txt \
#               -min_length 35 \
#               -verbose \
#               -stepwise_penalty \
#               -coverage_penalty summary \
#               -beta 5 \
#               -length_penalty wu \
#               -alpha 0.9 \
#               -verbose \
#               -block_ngram_repeat 3 \
#               -ignore_when_blocking "." "</t>" "<t>"