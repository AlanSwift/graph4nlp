graph_construction_name: "dependency"
graph_embedding_name: "gcn"
decoder_name: "stdrnn"

graph_construction_args:
  graph_construction_share:
    graph_type: 'dependency'
    root_dir: "graph4nlp/pytorch/test/dataset/jobs"
    topology_subdir: 'DependencyGraph'
    lower_case: true
    share_vocab: true
    input_language: "en"
    output_language: "en"
    thread_number: 4

  graph_construction_private:
    edge_strategy: 'homogeneous'
    merge_strategy: 'tailhead'
    sequential_link: true
    as_node: false
    nlp_tools_args: # we use stanford corenlp as default nlp tools, please refer to ``https://stanfordnlp.github.io/CoreNLP/tokenize.html``
      name: "stanfordcorenlp"
      tokenizer_args:
        whitespace: true # If set to true, separates words only when whitespace is encountered.
      normalize_parentheses: false # Whether to map round parentheses to -LRB-, -RRB-, as in the Penn Treebank
      normalize_other_brackets: false # Whether to map other common bracket characters to -LCB-, -LRB-, -RCB-, -RRB-, roughly as in the Penn Treebank
      split_hyphenated: false # Whether or not to tokenize hyphenated words as several tokens (“school” “-“ “aged”, “frog” “-“ “lipped”)
      is_oneSentence: true # Whether to regard the inputs as one sentence
      port: 9000
      timeout: 15000

  node_embedding:
    input_size: 300
    hidden_size: 300
    word_dropout: 0.2
    rnn_dropout: 0.3
    fix_bert_emb: false
    fix_word_emb: false
    embedding_style:
      single_token_item: true
      emb_strategy: "w2v_bilstm"
      num_rnn_layers: 1
      bert_model_name: null
      bert_lower_case: null

graph_embedding_args:
  graph_embedding_share:
    num_layers: 3
    input_size: 300
    hidden_size: 300
    output_size: 300
    direction_option: "undirected"
    feat_drop: 0.2

  graph_embedding_private:
    gcn_norm: 'both'
    weight: true
    bias: true
    activation: null
    allow_zero_in_degree: false
    use_edge_weight: false

decoder_args:
  rnn_decoder_share:
    rnn_type: "lstm"
    input_size: 300
    hidden_size: 512
    rnn_emb_input_size: 300
    use_copy: true
    use_coverage: true
    graph_pooling_strategy: "max"
    attention_type: "sep_diff_encoder_type"
    fuse_strategy: "concatenate"
    dropout: 0.3

  rnn_decoder_private:
    max_decoder_step: 50
    node_type_num: null
    tgt_emb_as_output_layer: true




