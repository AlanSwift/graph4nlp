# Data
dataset: 'squad-split2'
pre_word_emb_file: '/Users/hugo/Documents/Work/Research/Resources/glove-vectors/glove.840B.300d.txt' # path to the pretrained word embedding file
out_dir: 'out/squad_split2/ggnn_bi_fuse_constituency_ckpt'

# Preprocessing
top_word_vocab: 70000
min_word_freq: 1
n_samples: null


# Graph construction
graph_type: # 'dependency' # graph construction type ('dependency', 'constituency', 'ie', 'node_emb', 'node_emb_refined')
      - 'constituency'

# Dynamic graph construction
init_graph_type: null # initial graph construction type ('line', 'dependency', 'constituency', 'ie')
gl_metric_type: null # similarity metric type for dynamic graph construction ('weighted_cosine', 'attention', 'rbf_kernel', 'cosine')
gl_epsilon: null # epsilon for graph sparsification
gl_top_k: null # top k for graph sparsification
gl_num_heads: 1 # num of heads for dynamic graph construction
gl_num_hidden: 300 # number of hidden units for dynamic graph construction
gl_smoothness_ratio: null # smoothness ratio for graph regularization loss
gl_sparsity_ratio: null # sparsity ratio for graph regularization loss
gl_connectivity_ratio: null # connectivity ratio for graph regularization loss
init_adj_alpha: null # alpha ratio for combining initial graph adjacency matrix


# Graph embedding construction
emb_strategy: 'w2v_bilstm'
word_dropout: 0.4 # word embedding dropout
enc_rnn_dropout: 0.3 # Encoder RNN dropout
dec_rnn_dropout: 0.3 # Decoder RNN dropout
no_fix_word_emb: false # Not fix pretrained word embeddings (default: false)


# GNN
gnn: # 'ggnn'
      - 'ggnn'
gnn_direction_option: # 'bi_fuse' # GNN direction type ('undirected', 'bi_sep', 'bi_fuse')
      - 'bi_fuse'
gnn_num_layers: # 3 # 3 # number of GNN layers
      - 5
      - 4
      - 3
      - 2
num_hidden: 300 # number of hidden units
gnn_dropout: # 0.6 # GNN input feature dropout
      - 0.


# GAT
gat_attn_dropout: null # GAT attention dropout
gat_negative_slope: null # the negative slope of leaky relu
gat_num_heads: null # number of hidden attention heads
gat_num_out_heads: null # number of output attention heads
gat_residual: false # use gat_residual connection
# GraphSAGE
graphsage_aggreagte_type: null # graphsage aggreagte type ('mean', 'gcn', 'pool', 'lstm')


# Decoder
graph_pooling_strategy: 'max' # graph pooling strategy (null, 'mean', 'max', 'min')
dec_attention_type: # 'uniform' # decoder attention type ('uniform', 'sep_diff_encoder_type', 'sep_diff_node_type')
      - 'uniform'
      # - 'sep_diff_encoder_type'
dec_fuse_strategy: # 'concatenate' # the strategy to fuse attention results generated by separate attention ('average', 'concatenate')
      # - 'concatenate'
      - 'average'
use_coverage: True
coverage_loss_ratio: # 0.4 # coverage loss ratio
      - 0.4
tgt_emb_as_output_layer: True # When this option is set ``True``, the output projection layer(It is used to project RNN encoded representation to target sequence)'s weight will be shared with the target vocabulary's embedding.


# Training
seed: 1234
batch_size: 50 # batch size
epochs: 100 # number of maximal training epochs
grad_clipping: 10
early_stop_metric: 'BLEU_4'
patience: 10
lr: 0.001 # learning rate
lr_patience: 2
lr_reduce_factor: 0.5
num_workers: 2 # number of data loader workers


# Beam search
beam_size: 1
min_out_len: 4 # Only for beam search
max_dec_steps: 50 # Only for beam search

gpu: -1
no_cuda: false
